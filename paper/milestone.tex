\documentclass[11pt]{article}
\usepackage{cs229}

\title{TurnZero: Uncertainty-Aware Team Preview Prediction\\for Competitive Pok\'emon VGC}
\author{Lucas Erlebach-Astorga}

\begin{document}
\maketitle

\begin{abstract}
In competitive Pok\'emon VGC, each player privately selects 4 of 6 Pok\'emon to bring and 2 to lead---a ``team preview'' decision with $\binom{6}{4}\times\binom{4}{2}=90$ valid actions.
Because experts routinely disagree on the correct play, raw accuracy is a poor measure of model quality, making uncertainty quantification the central challenge.
I frame team preview as a 90-way classification problem over 382K expert replays and propose a calibrated decision support system built on deep ensembles of permutation-invariant transformers.
This milestone details the data pipeline, baseline experiments, and planned training methodology.
\end{abstract}


%=============================================================================
\section{Introduction}
\label{sec:intro}
%=============================================================================

Pok\'emon Video Game Championships (VGC) is the official doubles format for competitive Pok\'emon.
Under the Open Team Sheet (OTS) rule, both players see each other's full roster of 6 Pok\'emon---including moves, items, and abilities---before the battle starts.
Each player then privately selects which 4 to bring and which 2 to lead.
This ``team preview'' is the first strategic decision in a match, chosen from $\binom{6}{4}\times\binom{4}{2}=90$ valid actions.

What makes this interesting as a machine learning problem is its multi-modality: top players facing the same matchup regularly make different selections, and a reasonable argument exists for each.
The action space is large enough that any single prediction will usually be wrong, but small enough that a calibrated distribution over actions is useful to a human player.
This makes uncertainty quantification (UQ) the real challenge, not classification accuracy.

I propose TurnZero, a calibrated decision support system that treats team preview as 90-way classification with a full UQ stack.

\textbf{Planned contributions.}
\begin{itemize}
    \item A position-invariant transformer architecture trained on 382K expert replays, with popularity and logistic regression baselines.
    \item A UQ pipeline: deep ensemble training, reliability calibration, risk-coverage analysis, and out-of-distribution detection.
    \item Feature importance analysis identifying which OTS features (species, moves, items, abilities) drive predictions.
    \item Per-team predictability analysis investigating why some team archetypes are more predictable than others.
\end{itemize}


%=============================================================================
\section{Related Work}
\label{sec:related}
%=============================================================================

\textbf{Full-battle agents.}
VGC-Bench~\cite{vgcbench} defines the same 90-way team preview action space and trains transformer policies via behavior cloning and self-play on 705K OTS battle logs.
However, team preview is one incidental sub-action in a full-game RL pipeline---never isolated or evaluated separately, and no UQ is reported.
EliteFurretAI~\cite{elitefurretai} builds a 135M-parameter battle agent whose team preview head initially reported 99.9\% accuracy, later traced to positional leakage: Showdown logs store Pok\'emon in a fixed order, and 88.6\% of unique states mapped to exactly one action.
After random-order augmentation, accuracy dropped to 79\%.
Metamon~\cite{metamon} and PokeChamp~\cite{pokechamp} target Singles---a different format with no team preview decision.

\textbf{Lead prediction.}
Carli~\cite{carli2025} is the only prior work to specifically predict VGC leads, using SVD-based similarity matching on ${\sim}$5K logs with species-only features.
No formal evaluation metrics or UQ are reported.
This work differs in every dimension: supervised 90-way classification with full OTS features, 382K examples, and a planned uncertainty quantification stack.

\textbf{Set architectures.}
The permutation-invariant design draws on Deep Sets~\cite{deepsets} and Set Transformer~\cite{settransformer}, using self-attention over unordered token sets with mean-pool aggregation.


%=============================================================================
\section{Data and Problem Setup}
\label{sec:data}
%=============================================================================

\textbf{Action space.}
Each player chooses 4 of 6 Pok\'emon to bring ($\binom{6}{4} = 15$ options) and arranges 2 of those 4 as leads ($\binom{4}{2} = 6$ arrangements), yielding $15 \times 6 = 90$ distinct actions.
I encode each action as an integer $a \in \{0, \ldots, 89\}$ via a canonical bijection.

\textbf{Dataset.}
I extract 382K directed examples (two per game, one from each player's perspective) from 212K Regulation~G battles on Pok\'emon Showdown, sourced from the \texttt{vgc-battle-logs} dataset~\cite{vgcbench}.
Each example consists of two full Open Team Sheets---species, item, ability, tera type, and all four moves for each of 12 Pok\'emon---and a ground-truth action label.
Raw battle logs are parsed by extracting \texttt{|showteam|} protocol lines, which provide complete OTS information for 96.8\% of games.
Names are normalized from Showdown's CamelCase format to canonical display names, moves are lexicographically sorted within each Pok\'emon, and Pok\'emon are sorted within each team by a canonical key to ensure permutation invariance at the data level.

\textbf{Label observability.}
Leads are always visible in the log, but the full bring-4 is only observable when all four Pok\'emon appear during the game.
Roughly 80\% of examples have fully observed bring-4 (Tier~1); the remaining 20\% (Tier~2) have reliable lead labels but uncertain back-2 assignments.
All evaluation metrics will be computed on Tier~1 data only.

\textbf{Split design.}
Na\"ive random splitting risks data leakage: two examples from the same battle share identical team sheets, and common team archetypes recur across thousands of games.
I address this with a two-stage approach.
First, I cluster teams by species composition: teams sharing $\geq$4 of 6 species are connected via union-find, yielding 7,826 clusters from 116,903 unique teams.
Second, I construct two evaluation regimes.
\emph{Regime A} (in-distribution) holds out Team~A variants within each cluster, preserving archetype exposure during training; \emph{Regime B} (out-of-distribution) holds out entire clusters to test generalization to unseen team compositions.
Regime~A yields 247K train / 35K validation / 40K test examples, with all 90 actions represented in every split.
Exact \texttt{(team\_a, team\_b, action)} triples are deduplicated across splits, while the same matchup with different expert actions is preserved as signal (multi-modality), not leakage.
All rows sharing a \texttt{match\_group\_id} are confined to a single split.


%=============================================================================
\section{Methods}
\label{sec:methods}
%=============================================================================

\subsection{Baselines}

\textbf{Popularity baseline} (0 parameters).
For each test example, predict the global action frequency distribution from the training set.
This captures metagame conventions without any matchup-specific reasoning.

\textbf{Multinomial logistic regression} (${\sim}$4K parameters).
Bag-of-embeddings over the 12 Pok\'emon's discrete features (species, item, ability, tera type, 4 moves), passed through a single linear layer to 90-way softmax.

\subsection{Transformer Architecture}

Each of the 12 Pok\'emon is represented as a token---the sum of 8 learned embeddings (species, item, ability, tera type, 4 moves)---and processed by a 4-layer transformer encoder ($d{=}128$, $H{=}4$, $d_\text{ff}{=}512$, dropout 0.1), totaling 1.16M parameters.
The architecture is permutation-invariant by design: Pok\'emon within each team are canonically sorted, and the 12-token sequence is aggregated via mean pooling before a linear classification head.
No positional encoding is used---order carries no information and would introduce the leakage risk documented in EliteFurretAI~\cite{elitefurretai}.
Training uses cross-entropy loss, AdamW ($\text{lr}{=}3{\times}10^{-4}$), cosine annealing, and mixed-precision (BF16) on a single RTX 4080 Super.

\subsection{Planned: Deep Ensemble and Uncertainty Quantification}

I plan to train 5 independent transformer members with different random seeds~\cite{deepensembles}.
The ensemble prediction averages member softmax outputs: $\bar{p}(a \mid x) = \frac{1}{M}\sum_{m=1}^{M} p_m(a \mid x)$.
Predictive uncertainty will decompose into total uncertainty (entropy $\mathcal{H}[\bar{p}]$) and epistemic uncertainty (mutual information between ensemble members), enabling the model to distinguish inherently ambiguous matchups from ones where the model lacks knowledge.
Post-hoc temperature scaling~\cite{calibration} on the validation set will be used to improve calibration if needed.
For selective prediction, a confidence threshold $\tau$ will allow the model to abstain on low-confidence inputs, trading coverage for accuracy~\cite{selectivepred}.


%=============================================================================
\section{Preliminary Experiments}
\label{sec:experiments}
%=============================================================================

\begin{table}[t]
\centering
\caption{Baseline performance on the Regime A test set (Tier 1, $n{=}$32,328). Top-1/3 accuracy (\%, $\uparrow$), NLL ($\downarrow$), ECE ($\downarrow$).}
\label{tab:baselines}
\small
\begin{tabular}{@{}lrccc@{}}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Top-1} & \textbf{Top-3} & \textbf{NLL} \\
\midrule
Random & --- & 1.1 & 3.3 & 4.500 \\
Popularity & 0 & 1.3 & 3.9 & 4.497 \\
Logistic & $\sim$4K & 4.0 & 10.1 & 4.580 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:baselines} reports baseline results.
The popularity baseline barely exceeds random, confirming that team preview decisions are matchup-dependent rather than driven by global metagame frequencies.
Multinomial logistic regression achieves higher top-$k$ accuracy (4.0\% top-1 vs.\ 1.3\%), showing that even a linear model can learn some matchup-specific signal from the full OTS feature set.

However, a critical observation is that the logistic baseline achieves \emph{worse} NLL than the zero-parameter popularity baseline (4.580 vs.\ 4.497).
This occurs because the logistic model makes overconfident wrong predictions: when it assigns high probability to an incorrect action, the log-loss penalty is severe.
This finding validates the need for both a more flexible model (to capture nonlinear matchup interactions) and proper calibration (to ensure predicted probabilities are honest).
For a decision support tool where a player acts on the model's confidence scores, well-calibrated probabilities matter more than marginal accuracy gains.


%=============================================================================
\section{Next Steps}
\label{sec:nextsteps}
%=============================================================================

\begin{enumerate}
    \item \textbf{Transformer training.} Complete training of the permutation-invariant transformer and compare against baselines on top-$k$ accuracy, NLL, and ECE.
    \item \textbf{Deep ensemble.} Train 5 ensemble members with different seeds and evaluate whether ensembling improves calibration and selective prediction quality.
    \item \textbf{Calibration analysis.} Compute reliability diagrams and apply temperature scaling. Evaluate whether post-hoc calibration is necessary given ensemble averaging.
    \item \textbf{Risk-coverage analysis.} Plot risk-coverage curves and compute AURC to quantify the model's ability to ``know what it doesn't know.''
    \item \textbf{Feature ablation.} Systematically mask input features (moves, items, abilities, tera type) to identify which OTS components drive predictions.
    \item \textbf{OOD evaluation.} Compare Regime~A (in-distribution) vs.\ Regime~B (held-out clusters) to test whether the ensemble automatically becomes more cautious on unseen team archetypes.
    \item \textbf{Per-team analysis.} Investigate whether certain team archetypes (e.g., teams with the Commander ability) are systematically more predictable than others.
\end{enumerate}

\textbf{Contributions.}
Solo project. I designed the data pipeline, baselines, model architecture, and evaluation framework.

%=============================================================================
% References
%=============================================================================

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Angliss et~al.(2026)]{vgcbench}
Cameron Angliss, Giovanni Sutanto, and others.
\newblock VGC-Bench: Benchmarking AI agents in Pok\'emon VGC.
\newblock In \emph{Proceedings of AAMAS}, 2026.

\bibitem[Simpson(2024)]{elitefurretai}
Cameron Simpson.
\newblock EliteFurretAI: A competitive Pok\'emon battle agent.
\newblock GitHub repository, 2024.

\bibitem[Grigsby et~al.(2025)]{metamon}
Jake Grigsby, Yufei Wang, and Yuke Zhu.
\newblock Metamon: Learning to play Pok\'emon from offline data.
\newblock In \emph{Reinforcement Learning Conference}, 2025.

\bibitem[Karten et~al.(2025)]{pokechamp}
Seth Karten and others.
\newblock PokeChamp: LLM-enhanced minimax tree search for competitive Pok\'emon.
\newblock In \emph{ICML}, 2025.

\bibitem[Carli(2025)]{carli2025}
Joseph~A. Carli.
\newblock Predicting competitive Pok\'emon VGC leads using latent semantic analysis.
\newblock \emph{Journal of Geek Studies}, 2025.

\bibitem[Zaheer et~al.(2017)]{deepsets}
Manzil Zaheer, Saswat Kottur, Siamak Ravanbakhsh, Barnab{\'a}s P{\'o}czos, Russ Salakhutdinov, and Alexander Smola.
\newblock Deep Sets.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Lee et~al.(2019)]{settransformer}
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam~R. Kosiorek, Seungjin Choi, and Yee~Whye Teh.
\newblock Set Transformer: A framework for attention-based permutation-invariant input.
\newblock In \emph{ICML}, 2019.

\bibitem[Lakshminarayanan et~al.(2017)]{deepensembles}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Guo et~al.(2017)]{calibration}
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Geifman and El-Yaniv(2017)]{selectivepred}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selective classification for deep neural networks.
\newblock In \emph{NeurIPS}, 2017.

\end{thebibliography}

\end{document}
