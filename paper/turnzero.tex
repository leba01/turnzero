\documentclass[11pt]{article}
\usepackage{cs229}
\usepackage{subcaption}

\title{TurnZero: Uncertainty-Aware Team Preview Prediction\\for Competitive Pok\'emon VGC}
\author{Lucas Erlebach-Astorga}

\begin{document}
\maketitle

\begin{abstract}
In competitive Pok\'emon VGC, each player privately selects 4 of 6 Pok\'emon to bring and 2 to lead---a ``team preview'' decision with $\binom{6}{4}\times\binom{4}{2}=90$ valid actions.
Because experts routinely disagree on the correct play, raw accuracy is a poor measure of model quality.
I frame team preview as a 90-way classification problem and show that uncertainty quantification matters more than point accuracy in this multi-modal domain.
A 5-member deep ensemble of permutation-invariant transformers (1.16M params each) achieves 6.4\% top-1 on 32K test examples---modest alone, but $k{=}17$ predictions cover 50\% of outcomes (vs.\ $k{=}45$ random).
The ensemble is well-calibrated (ECE $= 0.011$), supports selective prediction (AURC $= 0.890$), and doubles abstention on out-of-distribution teams (20\%$\to$46\%).
Per-team analysis reveals a bimodal distribution: ``commander'' archetypes reach 50\% top-1, while flexible ``goodstuffs'' teams sit at 0\%. Ensemble entropy separates them ($r = {-}0.56$).
\end{abstract}


%=============================================================================
\section{Introduction}
\label{sec:intro}
%=============================================================================

Pok\'emon Video Game Championships (VGC) is the official doubles format for competitive Pok\'emon.
Under the Open Team Sheet (OTS) rule, both players see each other's full roster of 6 Pok\'emon---including moves, items, and abilities---before the battle starts.
Each player then privately selects which 4 to bring and which 2 to lead.
This ``team preview'' is the first strategic decision in a match, chosen from $\binom{6}{4}\times\binom{4}{2}=90$ valid actions.

What makes this interesting as ML is its multi-modality: top players facing the same matchup regularly make different selections, and a reasonable argument exists for each.
The action space is large enough that any single prediction will usually be wrong, but small enough that a calibrated distribution over actions is useful to a human player.
This makes uncertainty quantification (UQ) the real challenge, not classification accuracy.

I present TurnZero, a calibrated decision support system that treats team preview as 90-way classification with a full UQ stack: deep ensembles, selective prediction, and per-team entropy analysis.

\textbf{Contributions.}
\begin{itemize}
    \item A position-invariant transformer architecture trained on 382K expert replays, with popularity and logistic regression baselines.
    \item A complete UQ pipeline: 5-member deep ensemble, reliability calibration (ECE $= 0.011$), risk-coverage analysis (AURC $= 0.890$), and out-of-distribution detection via entropy shift.
    \item Per-team predictability analysis showing that aggregate accuracy masks a bimodal distribution driven by mechanical constraints, not strategic complexity.
    \item A 7-level feature ablation identifying moves as the dominant signal (masking all moves drops top-3 accuracy from 15.5\% to 3.2\%).
\end{itemize}


%=============================================================================
\section{Related Work}
\label{sec:related}
%=============================================================================

\textbf{Full-battle agents.}
VGC-Bench~\cite{vgcbench} defines the same 90-way team preview action space and trains transformer policies via behavior cloning and self-play on 705K OTS battle logs.
However, team preview is one incidental sub-action in a full-game RL pipeline---never isolated or evaluated separately, and no UQ is reported.
EliteFurretAI~\cite{elitefurretai} builds a 135M-parameter battle agent whose team preview head initially reported 99.9\% accuracy, later traced to positional leakage: Showdown logs store Pok\'emon in a fixed order, and 88.6\% of unique states mapped to exactly one action.
After random-order augmentation, accuracy dropped to 79\%.
The architecture avoids this by design via canonical sorting and mean pooling.
Metamon~\cite{metamon} and PokeChamp~\cite{pokechamp} target Singles---a different format with no team preview decision.

\textbf{Lead prediction.}
Carli~\cite{carli2025} is the only prior work to specifically predict VGC leads, using SVD-based similarity matching on ${\sim}$5K logs with species-only features.
No formal evaluation metrics, no UQ.
This work differs in every dimension: supervised 90-way classification with full OTS features, 382K examples, and a complete uncertainty quantification stack.

\textbf{Set architectures.}
The permutation-invariant design draws on Deep Sets~\cite{deepsets} and Set Transformer~\cite{settransformer}, using self-attention over unordered token sets with mean-pool aggregation.


%=============================================================================
\section{Data and Problem Setup}
\label{sec:data}
%=============================================================================

\textbf{Action space.}
Each player chooses 4 of 6 Pok\'emon to bring ($\binom{6}{4} = 15$ options) and arranges 2 of those 4 as leads ($\binom{4}{2} = 6$ arrangements), yielding $15 \times 6 = 90$ distinct actions.
I encode each action as an integer $a \in \{0, \ldots, 89\}$ via a canonical bijection.

\textbf{Dataset.}
I extract 382K directed examples (two per game, one from each player's perspective) from 212K Regulation~G battles on Pok\'emon Showdown, sourced from the \texttt{vgc-battle-logs} dataset~\cite{vgcbench}.
Each example consists of two full Open Team Sheets---species, item, ability, tera type, and all four moves for each of 12 Pok\'emon---and a ground-truth action label.

\textbf{Label observability.}
Leads are always visible in the log, but the full bring-4 is only observable when all four Pok\'emon appear during the game.
Roughly 80\% of examples have fully observed bring-4 (Tier~1); the remaining 20\% (Tier~2) have reliable lead labels but uncertain back-2 assignments.
All evaluation metrics are computed on Tier~1 data only.

\textbf{Split design.}
I cluster teams by species composition ($\geq$4-of-6 overlap $\to$ connected components via union-find, 7,826 clusters) and construct two regimes.
\emph{Regime A} (in-distribution) holds out Team~A variants within a cluster; \emph{Regime B} (OOD) holds out entire clusters.
Regime~A: 247K train / 35K val / 40K test, all 90 actions in every split.
The same matchup with different expert actions is signal (multi-modality), not leakage---only exact (team\_a, team\_b, action) triples are deduplicated across splits.


%=============================================================================
\section{Methods}
\label{sec:methods}
%=============================================================================

\subsection{Baselines}

\textbf{Popularity baseline} (0 parameters).
For each test example, predict the global action frequency distribution from the training set.
This captures metagame conventions without any matchup-specific reasoning.

\textbf{Multinomial logistic regression} (${\sim}$4K parameters).
Bag-of-embeddings over the 12 Pok\'emon's discrete features (species, item, ability, tera type, 4 moves), passed through a single linear layer to 90-way softmax.
The logistic baseline achieves higher top-1 accuracy than popularity (4.0\% vs.\ 1.3\%) but \emph{worse} NLL (4.580 vs.\ 4.497)---its overconfident wrong predictions are actively harmful for decision support.
This validates the need for both a flexible model and proper calibration.

\subsection{Transformer}

Each of the 12 Pok\'emon is represented as a token---the sum of 8 learned embeddings (species, item, ability, tera type, 4 moves)---and processed by a 4-layer transformer encoder ($d{=}128$, $H{=}4$, $d_\text{ff}{=}512$, dropout 0.1), totaling 1.16M parameters.
The architecture is permutation-invariant by design: Pok\'emon within each team are canonically sorted, and the 12-token sequence is aggregated via mean pooling before a linear classification head.
No positional encoding is used---order carries no information and would introduce the leakage risk documented in EliteFurretAI~\cite{elitefurretai}.
Training uses cross-entropy loss, AdamW ($\text{lr}{=}3{\times}10^{-4}$), cosine annealing, and mixed-precision (BF16) on a single RTX 4080 Super.
Each model converges in ${\sim}$23 epochs (${\sim}$5 minutes).

\subsection{Deep Ensemble and Uncertainty Decomposition}

I train 5 independent transformer members with different random seeds~\cite{deepensembles}.
The ensemble prediction is $\bar{p}(a \mid x) = \frac{1}{M}\sum_{m=1}^{M} p_m(a \mid x)$.
Predictive uncertainty decomposes into total uncertainty (entropy $\mathcal{H}[\bar{p}]$, capturing both data noise and model ignorance) and epistemic uncertainty (mutual information $\mathrm{MI} = \mathcal{H}[\bar{p}] - \frac{1}{M}\sum_m \mathcal{H}[p_m]$, isolating model disagreement).
High MI signals the ensemble members disagree---the model is uncertain, not just the problem being hard.
Post-hoc temperature scaling~\cite{calibration} yielded $T = 1.158$ (near-identity), confirming the ensemble is already well-calibrated; it is omitted from the final pipeline.

\subsection{Selective Prediction}

Given a confidence threshold $\tau$, the model abstains on inputs where $\max_a \bar{p}(a) < \tau$, trading coverage for accuracy~\cite{selectivepred}.
The risk-coverage curve plots error rate against the fraction of examples predicted on; its area (AURC) summarizes selective prediction quality (lower is better).


%=============================================================================
\section{Results}
\label{sec:results}
%=============================================================================

\subsection{Main Comparison}

\begin{table}[t]
\centering
\caption{Test set performance (Regime A, Tier 1, $n{=}$32,328). Top-1/3 accuracy (\%, $\uparrow$), NLL ($\downarrow$), ECE ($\downarrow$). Bootstrap 95\% CIs for the ensemble are cluster-aware ($B{=}$1000). The logistic baseline's worse NLL than popularity reflects overconfident wrong predictions.}
\label{tab:main}
\small
\begin{tabular}{@{}lrcccc@{}}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Top-1} & \textbf{Top-3} & \textbf{NLL} & \textbf{ECE} \\
\midrule
Random & --- & 1.1 & 3.3 & 4.500 & --- \\
Popularity & 0 & 1.3 & 3.9 & 4.497 & 0.001 \\
Logistic & $\sim$4K & 4.0 & 10.1 & 4.580 & 0.059 \\
Transformer & 1.16M & 5.5 & 14.0 & 4.105 & 0.016 \\
\textbf{Ensemble (5)} & \textbf{5.8M} & \textbf{6.4} & \textbf{15.5} & \textbf{4.031} & \textbf{0.011} \\
\midrule
\multicolumn{2}{@{}l}{\footnotesize Ensemble 95\% CI} & \footnotesize [2.6, 6.6] & \footnotesize [6.8, 16.0] & \footnotesize [4.01, 4.43] & \footnotesize [0.002, 0.013] \\
\bottomrule
\end{tabular}
\end{table}

The ensemble achieves 6.4\% top-1 and 15.5\% top-3 on the 90-way action space (5.8$\times$ and 4.7$\times$ random).
NLL of 4.031 (vs.\ 4.497 popularity) confirms the model learns matchup-specific signal beyond metagame frequencies, and ECE of 0.011 means the confidence scores are honest.
The wide bootstrap CIs (top-1: [2.6\%, 6.6\%]) reflect genuine heterogeneity across team archetypes, explored in Section~\ref{sec:perteam}.

\subsection{Reframing Accuracy: Top-k Coverage}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.68\columnwidth]{figures/topk_accuracy_curve.pdf}
    \caption{Cumulative coverage vs.\ $k$. The ensemble reaches 50\% at $k{=}17$ (vs.\ $k{=}45$ random), concentrating mass on the right region of the action space.}
    \label{fig:topk}
\end{figure}

Top-1 accuracy undersells the model because the task is multi-modal---there are often several reasonable plays.
Figure~\ref{fig:topk} shows that the ensemble needs only $k{=}17$ predictions to cover 50\% of expert actions (vs.\ $k{=}45$ for a uniform predictor).
By $k{=}10$, coverage already exceeds 36\%.
The marginal decomposition clarifies where difficulty concentrates: bring-4 and lead-2 marginals each achieve ${\sim}$18\% top-1, but conditional on a correct bring-4, the model picks the right lead pair only 32\% of the time (2$\times$ the 16.7\% random baseline for a 6-way choice).
Teams agree on \emph{what} to bring but disagree about \emph{who leads}.


\subsection{Calibration and Selective Prediction}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\textwidth]{figures/reliability_diagram.pdf}
        \caption{Reliability diagram. ECE $= 0.011$.}
        \label{fig:reliability}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\columnwidth}
        \includegraphics[width=\textwidth]{figures/risk_coverage_top1.pdf}
        \caption{Risk-coverage curve (top-1).}
        \label{fig:riskcov}
    \end{subfigure}
    \caption{\textbf{(a)} Predicted confidence closely tracks observed accuracy---the ensemble's probabilities mean what they say. \textbf{(b)} By abstaining on low-confidence predictions, the ensemble achieves lower error at every coverage level. AURC: 0.890 (ensemble) vs.\ 0.905 (single) vs.\ 0.953 (logistic).}
    \label{fig:calibration}
\end{figure}

Figure~\ref{fig:reliability} shows that predicted confidence closely tracks observed accuracy across all bins.
For a decision support tool, this matters more than raw accuracy: a player can trust that a 15\% prediction is roughly a 1-in-7 chance.
Figure~\ref{fig:riskcov} shows the risk-coverage tradeoff.
The ensemble's AURC of 0.890 (vs.\ 0.905 single, 0.953 logistic) confirms that ensembling improves not just accuracy but the model's ability to rank its own predictions by reliability.

\textbf{OOD behavior.}
On Regime~B (held-out clusters), abstention jumps from 20\% to 46\%---the ensemble automatically becomes more cautious on unfamiliar teams.
Top-1 accuracy on non-abstained Regime~B examples is \emph{higher} (11.7\% vs.\ 6.4\%): OOD teams that survive the confidence filter tend to be distinctive archetypes with fewer viable strategies.
Calibration degrades on OOD data (ECE: $0.011 \to 0.076$).

\subsection{Per-Team Predictability}
\label{sec:perteam}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.68\columnwidth]{figures/cluster_entropy_vs_accuracy.pdf}
    \caption{Mean ensemble entropy vs.\ top-3 accuracy for 153 team archetypes ($\geq$20 Tier~1 examples). Point size $\propto n$. $r = {-}0.56$: low-entropy ``commander'' teams reach 50\% top-1; high-entropy ``goodstuffs'' sit at 0\%.}
    \label{fig:entropy_scatter}
\end{figure}

The aggregate 6.4\% top-1 average obscures a bimodal distribution.
Figure~\ref{fig:entropy_scatter} groups the test set by species composition (153 archetypes with $\geq$20 examples) and plots each team's mean ensemble entropy against its accuracy.
The correlation is strong ($r = {-}0.56$ for top-3) and domain-plausible:
the most predictable teams are ``commander'' archetypes (e.g., Dondozo + Tatsugiri, $H = 3.10$, 50\% top-1) where the Commander ability mechanically requires both Pok\'emon to lead together.
The least predictable are flexible ``goodstuffs'' teams with many viable configurations ($H = 4.41$, 0\% top-1).

Speed control mode (Trick Room vs.\ Tailwind) does \emph{not} explain this: the entropy gap is 0.016 nats---effectively zero.
Predictability is driven by mechanical constraints (Commander ability), not strategic choices.

\subsection{Feature Importance}

A 7-level feature masking stress test confirms that moves dominate the signal: masking all moves drops top-3 from 15.5\% to 3.2\% (near the popularity baseline's 3.9\%), while items or tera type alone cost ${\sim}$3pp each.
Species alone still beats random, confirming composition-level patterns.
The mirror/non-mirror gap (6.8\% vs.\ 3.8\% top-1) further shows the model's reliance on convention---common archetypes have established lead patterns that novel matchups lack.


%=============================================================================
\section{Discussion}
\label{sec:discussion}
%=============================================================================

\textbf{Multi-modality is the ceiling.}
The challenge is not model capacity but intrinsic label disagreement: teams converge on which 4 to bring (17.8\% top-1 over 15 classes) but diverge on lead arrangement (32\% conditional accuracy over 6 classes, just 2$\times$ random).

\textbf{Convention, not strategy.}
The mirror/non-mirror gap (6.8\% vs.\ 3.8\% top-1) and entropy-accuracy correlation point to the same conclusion: the model learns what experts \emph{typically} do, not what they \emph{should} do.
Valuable for decision support, but not strategic reasoning.

\textbf{Engineering matters.}
The leakage-safe split (avoiding the 99.9\%$\to$79\% collapse documented in~\cite{elitefurretai}), Tier~1/2 delineation, and cluster-aware bootstrap all had outsized impact.

\textbf{Limitations.}
Bootstrap CIs are wide (top-1: [2.6\%, 6.6\%]). Calibration degrades OOD (ECE: $0.011 \to 0.076$). Training data is ladder games, not tournament best-of-three.

\textbf{Future work.}
Conformal prediction sets would provide coverage guarantees with adaptive set sizes.
Opponent-conditional analysis could test whether linear teams are truly opponent-invariant.

\textbf{Contributions.}
Solo project. I designed the pipeline, models, evaluation, figures, and paper.
Code: \url{https://github.com/leba01/turnzero}


%=============================================================================
% References
%=============================================================================

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Angliss et~al.(2026)]{vgcbench}
Cameron Angliss, Giovanni Sutanto, and others.
\newblock VGC-Bench: Benchmarking AI agents in Pok\'emon VGC.
\newblock In \emph{Proceedings of AAMAS}, 2026.

\bibitem[Simpson(2024)]{elitefurretai}
Cameron Simpson.
\newblock EliteFurretAI: A competitive Pok\'emon battle agent.
\newblock GitHub repository, 2024.

\bibitem[Grigsby et~al.(2025)]{metamon}
Jake Grigsby, Yufei Wang, and Yuke Zhu.
\newblock Metamon: Learning to play Pok\'emon from offline data.
\newblock In \emph{Reinforcement Learning Conference}, 2025.

\bibitem[Karten et~al.(2025)]{pokechamp}
Seth Karten and others.
\newblock PokeChamp: LLM-enhanced minimax tree search for competitive Pok\'emon.
\newblock In \emph{ICML}, 2025.

\bibitem[Carli(2025)]{carli2025}
Joseph~A. Carli.
\newblock Predicting competitive Pok\'emon VGC leads using latent semantic analysis.
\newblock \emph{Journal of Geek Studies}, 2025.

\bibitem[Zaheer et~al.(2017)]{deepsets}
Manzil Zaheer, Saswat Kottur, Siamak Ravanbakhsh, Barnab{\'a}s P{\'o}czos, Russ Salakhutdinov, and Alexander Smola.
\newblock Deep Sets.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Lee et~al.(2019)]{settransformer}
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam~R. Kosiorek, Seungjin Choi, and Yee~Whye Teh.
\newblock Set Transformer: A framework for attention-based permutation-invariant input.
\newblock In \emph{ICML}, 2019.

\bibitem[Lakshminarayanan et~al.(2017)]{deepensembles}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Guo et~al.(2017)]{calibration}
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Geifman and El-Yaniv(2017)]{selectivepred}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selective classification for deep neural networks.
\newblock In \emph{NeurIPS}, 2017.

\end{thebibliography}

\end{document}
